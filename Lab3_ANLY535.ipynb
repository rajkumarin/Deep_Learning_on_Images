{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3_ANLY535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\dhanapr\\documents\\anaconda3\\lib\\site-packages (4.5.3.56)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\dhanapr\\documents\\anaconda3\\lib\\site-packages (from opencv-python) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True: \n",
    "    ret, frame= cap.read() # Forever it returns the frame and ret which is false or true \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #if you want to convert the color \n",
    "    cv2.imshow('frame', frame) \n",
    "    cv2.imshow('gray', gray) # to show the gray video \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # If q is pressed stop \n",
    "        break \n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-Try to understand each line of the code and explain it in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We create a variable 'cap' in our environment that gets input from the PC's webcam number #0 using the cv2 package. \n",
    "- We put an infinite processing loop to extract each frame coming into variable 'cap'\n",
    "- Inside the loop, we convert the RGB image to grayscale\n",
    "- Inside the loop, we display both the color image and the gray on separate windows using cv2 package\n",
    "- We specify a method to quit the infinite loop\n",
    "- Once quit from the loop, we disconnect the webcam input and close the windows opened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2-Use the code provided (mnist-cnn) to design a digit recognition but this time read the image of the number directly from your webcam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Epoch 1/12\n",
      "375/375 [==============================] - 32s 85ms/step - loss: 0.2858 - accuracy: 0.9114 - val_loss: 0.0787 - val_accuracy: 0.9790\n",
      "Epoch 2/12\n",
      "375/375 [==============================] - 33s 87ms/step - loss: 0.1025 - accuracy: 0.9688 - val_loss: 0.0566 - val_accuracy: 0.9830\n",
      "Epoch 3/12\n",
      "375/375 [==============================] - 32s 87ms/step - loss: 0.0765 - accuracy: 0.9771 - val_loss: 0.0449 - val_accuracy: 0.9875\n",
      "Epoch 4/12\n",
      "375/375 [==============================] - 33s 88ms/step - loss: 0.0602 - accuracy: 0.9811 - val_loss: 0.0428 - val_accuracy: 0.9874\n",
      "Epoch 5/12\n",
      "375/375 [==============================] - 33s 88ms/step - loss: 0.0502 - accuracy: 0.9845 - val_loss: 0.0427 - val_accuracy: 0.9883\n",
      "Epoch 6/12\n",
      "375/375 [==============================] - 33s 88ms/step - loss: 0.0442 - accuracy: 0.9857 - val_loss: 0.0411 - val_accuracy: 0.9897\n",
      "Epoch 7/12\n",
      "375/375 [==============================] - 33s 87ms/step - loss: 0.0402 - accuracy: 0.9867 - val_loss: 0.0425 - val_accuracy: 0.9885\n",
      "Epoch 8/12\n",
      "375/375 [==============================] - 32s 87ms/step - loss: 0.0348 - accuracy: 0.9889 - val_loss: 0.0403 - val_accuracy: 0.9893\n",
      "Epoch 9/12\n",
      "375/375 [==============================] - 35s 93ms/step - loss: 0.0347 - accuracy: 0.9888 - val_loss: 0.0371 - val_accuracy: 0.9899\n",
      "Epoch 10/12\n",
      "375/375 [==============================] - 33s 87ms/step - loss: 0.0305 - accuracy: 0.9904 - val_loss: 0.0426 - val_accuracy: 0.9887\n",
      "Epoch 11/12\n",
      "375/375 [==============================] - 32s 85ms/step - loss: 0.0267 - accuracy: 0.9911 - val_loss: 0.0414 - val_accuracy: 0.9889\n",
      "Epoch 12/12\n",
      "375/375 [==============================] - 32s 85ms/step - loss: 0.0251 - accuracy: 0.9914 - val_loss: 0.0394 - val_accuracy: 0.9902\n",
      "Test loss: 0.030806198716163635\n",
      "Test accuracy: 0.9915000200271606\n"
     ]
    }
   ],
   "source": [
    "# To train the model:\n",
    "%run \"C:/Users/dhanapr/Documents/GitHub/Deep_Learning_on_Images/data/mnist_cnn\"\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow\n",
    "new_model = tensorflow.keras.models.load_model('C:/Users/dhanapr/Documents/GitHub/Deep_Learning_on_Images/data/my_model.h5')\n",
    "# Handwritten recognition using camera\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_img_contour_thresh(img):\n",
    "    x, y, w, h = 0, 0, 300, 300\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (35, 35), 0)\n",
    "    ret, thresh1 = cv2.threshold(blur, 70, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    thresh1 = thresh1[y:y + h, x:x + w]\n",
    "    contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "    return img, contours, thresh1\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret, img = cap.read()\n",
    "    ret\n",
    "    img, contours, thresh = get_img_contour_thresh(img)\n",
    "    ans1 = ''\n",
    "    if len(contours) > 0:\n",
    "        contour = max(contours, key=cv2.contourArea)\n",
    "        if cv2.contourArea(contour) > 2500:\n",
    "            # print(predict(w_from_model,b_from_model,contour))\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            # newImage = thresh[y - 15:y + h + 15, x - 15:x + w +15]\n",
    "            newImage = thresh[y:y + h, x:x + w]\n",
    "            newImage = cv2.resize(newImage, (28, 28))\n",
    "            newImage = np.array(newImage)\n",
    "            newImage = newImage.flatten()\n",
    "            newImage = newImage.reshape(newImage.shape[0], 1)\n",
    "            newImage2 = newImage.flatten().reshape(1,28,28,1)\n",
    "            newImage2 = newImage2.astype('float32')\n",
    "            newImage2 /= 255\n",
    "            result = new_model.predict(newImage2)\n",
    "            ans1= np.argmax(result)\n",
    "            #ans1 = Digit_Recognizer_LR.predict(w_LR, b_LR, newImage)\n",
    "\n",
    "    x, y, w, h = 0, 0, 300, 300\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.putText(img, \"Prediction : \" + str(ans1), (10, 320),  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", img)\n",
    "    cv2.imshow(\"Contours\", thresh)\n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.3\n",
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('C:/Users/dhanapr/Documents/GitHub/Deep_Learning_on_Images/data/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/dhanapr/Documents/GitHub/Deep_Learning_on_Images/data/haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = img[y:y+h, x:x+w]\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "            for (ex,ey,ew,eh) in eyes:\n",
    "                cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "    cv2.imshow('Lab 3 Face recognition',img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\dhanapr\\Documents\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\dhanapr\\Documents\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# Just load 5000 cases \n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3-We need to do word embedding at this point. What is the meaning of “word embedding” in context of NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word Embedding is forming word clouds from a group of words such that synonyms and words of similar meaning or idea are close to each other. \n",
    "- Words that convey drastically different ideas will be placed far apart. \n",
    "- The distance between any two words in this cloud can be used to find how related or unrelated they are.\n",
    "- This representation helps in NLP tasks like sentiment analysis and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "391/391 [==============================] - 184s 471ms/step - loss: 0.5325 - accuracy: 0.7408 - val_loss: 0.3723 - val_accuracy: 0.8412\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 191s 489ms/step - loss: 0.3242 - accuracy: 0.8689 - val_loss: 0.3728 - val_accuracy: 0.8317\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 194s 496ms/step - loss: 0.3611 - accuracy: 0.8484 - val_loss: 0.3697 - val_accuracy: 0.8537\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 196s 501ms/step - loss: 0.2512 - accuracy: 0.9023 - val_loss: 0.3417 - val_accuracy: 0.8541\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 195s 498ms/step - loss: 0.2237 - accuracy: 0.9147 - val_loss: 0.3515 - val_accuracy: 0.8693\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 197s 505ms/step - loss: 0.2101 - accuracy: 0.9208 - val_loss: 0.3527 - val_accuracy: 0.8685\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 343s 878ms/step - loss: 0.1675 - accuracy: 0.9378 - val_loss: 0.3404 - val_accuracy: 0.8647\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 690s 2s/step - loss: 0.1390 - accuracy: 0.9498 - val_loss: 0.3584 - val_accuracy: 0.8621\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 617s 2s/step - loss: 0.1608 - accuracy: 0.9374 - val_loss: 0.4014 - val_accuracy: 0.8552\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 573s 1s/step - loss: 0.1515 - accuracy: 0.9438 - val_loss: 0.3980 - val_accuracy: 0.8556\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 395s 1s/step - loss: 0.0998 - accuracy: 0.9645 - val_loss: 0.4573 - val_accuracy: 0.8672\n",
      "Epoch 12/20\n",
      "116/391 [=======>......................] - ETA: 6:30 - loss: 0.0735 - accuracy: 0.9766"
     ]
    }
   ],
   "source": [
    "# design model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "hist= model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4-Draw the learning curves and describe them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5-Add a dropout to see how the model changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout to model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "hist= model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64)\n",
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6-Add the CNN layer and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7-Now write a DNN for  MNIST using Pytorch and completely explain your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "%run \"C:/Users/dhanapr/Documents/GitHub/Deep_Learning_on_Images/data/pytorch_mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
